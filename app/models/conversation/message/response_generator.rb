class Conversation::Message::ResponseGenerator
  include Ai::Prompts

  PROMPT = <<~PROMPT
    You are **Fizzy**, a helpful assistant for the Fizzy app by 37signals.
    Fizzy is a bug tracker / task manager for teams, and you help users manage their cards, collections, and team activity.

    ### ðŸ§  Your Role
    You help users with anything related to Fizzy â€” their cards, collections, trends, and team activity.

    You have several **tools** at your disposal to answer questions and perform actions.
    Use them freely when needed, especially when the answer depends on real data.

    ### âœ… Guidelines
    - Be **concise**, **accurate**, and **friendly**
    - Speak naturally â€” no corporate tone or robotic phrasing
    - **Never suggest follow-up questions, extra details, or further actions** unless the user explicitly asks
    - Do **not** include phrases like â€œIf you want moreâ€¦â€ or â€œLet me know ifâ€¦â€ â€” just answer the question as asked
    - Stick strictly to the user's intent â€” no speculation, hedging, or filler
    - When in doubt, examine their cards, collections, or team activity to figure out the answer.
    - If you're unsure what they mean, ask a clarifying question â€” but only if you truly cannot infer it from context
    - Always assume questions are about **their own Fizzy data** â€” cards, collections, or team activity
    - If a question isnâ€™t related to Fizzy, respond politely with â€œI donâ€™t knowâ€ or â€œIâ€™m not sureâ€ and explain that you can only answer questions related to Fizzy
    - Donâ€™t explain concepts or go off-topic â€” answer only what was asked
    - Respond in **Markdown**
    - Always include links to relevant cards, collections, comments, or users

    Remember, you're here to help â€” not to anticipate.
  PROMPT

  attr_reader :message, :prompt, :llm_model

  delegate :conversation, to: :message

  def initialize(message, prompt: PROMPT, llm_model: nil)
    @message = message
    @prompt = prompt
    @llm_model = llm_model
  end

  def generate
    reset_token_counters

    response = llm.ask(message.content.to_plain_text)
    answer = markdown_to_html(response.content)

    Response.new(
      answer: answer,
      input_tokens: input_tokens,
      output_tokens: output_tokens,
      model_id: response.model_id
    )
  end

  private
    attr_reader :input_tokens, :output_tokens

    def reset_token_counters
      @input_tokens = 0
      @output_tokens = 0
    end

    def llm
      RubyLLM.chat(model: llm_model).tap do |chat|
        chat.with_tool(Ai::Tool::ListCards.new)
        chat.with_tool(Ai::Tool::ListCollections.new)
        chat.with_tool(Ai::Tool::ListComments.new)
        chat.with_tool(Ai::Tool::ListStatusChanges.new)
        chat.with_tool(Ai::Tool::ListUsers.new)

        chat.reset_messages!

        previous_messages.each do |message|
          chat.add_message(message.to_llm)
        end

        chat.with_instructions join_prompts(prompt, domain_model_prompt, user_data_injection_prompt)

        track_token_usage_of_intermediate_messages(chat)
      end
    end

    def previous_messages
      conversation.messages.order(id: :asc).where(id: ...message.id).limit(50).with_rich_text_content
    end

    def track_token_usage_of_intermediate_messages(chat)
      chat.on_end_message do |response|
        @input_tokens = response.input_tokens
        @output_tokens = response.output_tokens
      end
    end

    def markdown_to_html(markdown)
      renderer = Redcarpet::Render::HTML.new
      markdowner = Redcarpet::Markdown.new(renderer, autolink: true, tables: true, fenced_code_blocks: true, strikethrough: true, superscript: true)
      markdowner.render(markdown).html_safe
    end
end
